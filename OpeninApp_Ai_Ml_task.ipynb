{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "nqXlZlKkMtMI",
        "yyQu1yzpFlsL",
        "ua_ewA1FVQK6",
        "16JjSoPuKYzP",
        "Bc22ad_6GDEC",
        "d4Y1L2BE_8ur",
        "R3dBraiZ9pdf",
        "_NuAj4CN_5bn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# installing packages"
      ],
      "metadata": {
        "id": "nqXlZlKkMtMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cmake\n",
        "!pip install ffmpeg-python\n",
        "!pip install dlib\n",
        "!pip install face_recognition\n",
        "!pip install moviepy\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"\\nDone\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M03HA-cKNIrk",
        "outputId": "40517631-18e4-4ad6-c864-90fc6b45bc84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trimming face and no face video and trimming audio"
      ],
      "metadata": {
        "id": "yyQu1yzpFlsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st method for face detection (getting few frame in face video with no face)"
      ],
      "metadata": {
        "id": "ua_ewA1FVQK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import face_recognition\n",
        "import numpy as np\n",
        "import moviepy.editor as mp\n",
        "\n",
        "# Load the video\n",
        "video_path = \"/content/input_video.mp4\"  # Update with your video file path\n",
        "audio_path = \"/content/input_audio.wav\"  # Update with your audio file path\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Create lists to store frames and timestamps\n",
        "frames_with_faces = []\n",
        "timestamps_with_faces = []\n",
        "frames_without_faces = []\n",
        "timestamps_without_faces = []\n",
        "\n",
        "# # Enable CUDA for GPU acceleration\n",
        "# cv2.cuda.setDevice(0)  # Set the GPU device number, if multiple GPUs are available\n",
        "\n",
        "while True:\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to RGB for face_recognition library\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # # Transfer frame data to GPU memory\n",
        "    # gpu_frame = cv2.cuda_GpuMat()\n",
        "    # gpu_frame.upload(rgb_frame)\n",
        "\n",
        "    # Detect faces in the frame using GPU-accelerated face recognition\n",
        "    face_locations = face_recognition.face_locations(rgb_frame)\n",
        "\n",
        "    if len(face_locations) > 0:\n",
        "        # Store frames with faces and corresponding timestamps\n",
        "        frames_with_faces.append(frame)\n",
        "        timestamps_with_faces.append(video.get(cv2.CAP_PROP_POS_MSEC) / 1000.0)\n",
        "    else:\n",
        "        # Store frames without faces and corresponding timestamps\n",
        "        frames_without_faces.append(frame)\n",
        "        timestamps_without_faces.append(video.get(cv2.CAP_PROP_POS_MSEC) / 1000.0)\n",
        "\n",
        "# Create video writer for frames with faces\n",
        "writer_with_faces = cv2.VideoWriter(\"/content/video_with_faces.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (frames_with_faces[0].shape[1], frames_with_faces[0].shape[0]))\n",
        "for frame in frames_with_faces:\n",
        "    writer_with_faces.write(frame)\n",
        "writer_with_faces.release()\n",
        "\n",
        "# Create video writer for frames without faces\n",
        "writer_without_faces = cv2.VideoWriter(\"/content/video_without_faces.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (frames_without_faces[0].shape[1], frames_without_faces[0].shape[0]))\n",
        "for frame in frames_without_faces:\n",
        "    writer_without_faces.write(frame)\n",
        "writer_without_faces.release()\n",
        "\n",
        "\n",
        "# Save timestamps of frames with faces\n",
        "np.savetxt(\"timestamps_with_faces.txt\", timestamps_with_faces)\n",
        "\n",
        "# Save timestamps of frames without faces\n",
        "np.savetxt(\"timestamps_without_faces.txt\", timestamps_without_faces)\n",
        "\n",
        "\n",
        "# Release the video object\n",
        "video.release()"
      ],
      "metadata": {
        "id": "GB4erCpn96G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import moviepy.editor as mp\n",
        "\n",
        "audio_path = \"/content/input_audio.wav\"  # Update with your audio file path\n",
        "\n",
        "# Trim audio based on timestamps of frames with faces\n",
        "audio_with_faces = mp.AudioFileClip(audio_path).subclip(timestamps_with_faces[0], timestamps_with_faces[-1])\n",
        "audio_with_faces.write_audiofile(\"audio_with_faces.mp3\")\n",
        "\n",
        "# Trim audio based on timestamps of frames without faces\n",
        "audio_without_faces = mp.AudioFileClip(audio_path).subclip(timestamps_without_faces[0], timestamps_without_faces[-1])\n",
        "audio_without_faces.write_audiofile(\"audio_without_faces.mp3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3AHe9R9-jv3",
        "outputId": "f6b8f862-ee80-4314-8ac6-f752611e9f7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in audio_with_faces.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "MoviePy - Writing audio in audio_without_faces.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2nd method for face detection working fine, (but still need some tuning)"
      ],
      "metadata": {
        "id": "16JjSoPuKYzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import face_recognition\n",
        "import numpy as np\n",
        "import moviepy.editor as mp\n",
        "\n",
        "# Load the video\n",
        "video_path = \"/content/input_video.mp4\"  # Update with your video file path\n",
        "audio_path = \"/content/input_audio.wav\"  # Update with your audio file path\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Create lists to store frames and timestamps\n",
        "frames_with_faces = []\n",
        "timestamps_with_faces = []\n",
        "frames_without_faces = []\n",
        "timestamps_without_faces = []\n",
        "\n",
        "# # Enable CUDA for GPU acceleration\n",
        "# cv2.cuda.setDevice(0)  # Set the GPU device number, if multiple GPUs are available\n",
        "\n",
        "# Load Haar cascade classifier for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# Parameters for face detection\n",
        "scale_factor = 1.3  # Adjust the scale factor for fine-tuning face detection sensitivity\n",
        "min_neighbors = 5  # Adjust the minimum number of neighbors for fine-tuning face detection accuracy\n",
        "min_size = (30, 30)  # Adjust the minimum face size for fine-tuning face detection\n",
        "\n",
        "while True:\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to grayscale for face detection\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # # Transfer frame data to GPU memory\n",
        "    # gpu_frame = cv2.cuda_GpuMat()\n",
        "    # gpu_frame.upload(rgb_frame)\n",
        "\n",
        "    # # Detect faces in the frame using GPU-accelerated face recognition\n",
        "    # face_locations = face_recognition.face_locations(gray_frame)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=scale_factor, minNeighbors=min_neighbors, minSize=min_size)\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        # Store frames with faces and corresponding timestamps\n",
        "        frames_with_faces.append(frame)\n",
        "        timestamps_with_faces.append(video.get(cv2.CAP_PROP_POS_MSEC) / 1000.0)\n",
        "    else:\n",
        "        # Store frames without faces and corresponding timestamps\n",
        "        frames_without_faces.append(frame)\n",
        "        timestamps_without_faces.append(video.get(cv2.CAP_PROP_POS_MSEC) / 1000.0)\n",
        "\n",
        "# Create video writer for frames with faces\n",
        "writer_with_faces = cv2.VideoWriter(\"/content/video_with_faces_gray.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (frames_with_faces[0].shape[1], frames_with_faces[0].shape[0]))\n",
        "for frame in frames_with_faces:\n",
        "    writer_with_faces.write(frame)\n",
        "writer_with_faces.release()\n",
        "\n",
        "# Create video writer for frames without faces\n",
        "writer_without_faces = cv2.VideoWriter(\"/content/video_without_faces_gray.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (frames_without_faces[0].shape[1], frames_without_faces[0].shape[0]))\n",
        "for frame in frames_without_faces:\n",
        "    writer_without_faces.write(frame)\n",
        "writer_without_faces.release()\n",
        "\n",
        "# Save timestamps of frames with faces\n",
        "np.savetxt(\"timestamps_with_faces_gray.txt\", timestamps_with_faces)\n",
        "\n",
        "# Save timestamps of frames without faces\n",
        "np.savetxt(\"timestamps_without_faces_gray.txt\", timestamps_without_faces)\n",
        "\n",
        "# Trim audio based on timestamps of frames with faces\n",
        "audio_with_faces = mp.AudioFileClip(audio_path).subclip(timestamps_with_faces[0], timestamps_with_faces[-1])\n",
        "audio_with_faces.write_audiofile(\"audio_with_faces_gray.mp3\")\n",
        "\n",
        "# Trim audio based on timestamps of frames without faces\n",
        "audio_without_faces = mp.AudioFileClip(audio_path).subclip(timestamps_without_faces[0], timestamps_without_faces[-1])\n",
        "audio_without_faces.write_audiofile(\"audio_without_faces_gray.mp3\")\n",
        "\n",
        "# Release the video object\n",
        "video.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1dVIm1KKXer",
        "outputId": "082e2672-b1f3-46ca-c475-c2b552f4dc48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in audio_with_faces_gray.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "MoviePy - Writing audio in audio_without_faces_gray.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <h1>audio trimmer</h1>\n",
        "import moviepy.editor as mp\n",
        "\n",
        "audio_path = \"/content/input_audio.wav\"  # Update with your audio file path\n",
        "\n",
        "# Trim audio based on timestamps of frames with faces\n",
        "audio_with_faces = mp.AudioFileClip(audio_path).subclip(timestamps_with_faces[0], timestamps_with_faces[-1])\n",
        "audio_with_faces.write_audiofile(\"audio_with_faces_gray.mp3\")\n",
        "\n",
        "# Trim audio based on timestamps of frames without faces\n",
        "audio_without_faces = mp.AudioFileClip(audio_path).subclip(timestamps_without_faces[0], timestamps_without_faces[-1])\n",
        "audio_without_faces.write_audiofile(\"audio_without_faces_gray.mp3\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3oq3WR4zR7qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# putting audio on face and no face library (optional)"
      ],
      "metadata": {
        "id": "Bc22ad_6GDEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set audio for video with faces\n",
        "video_with_faces = mp.VideoFileClip(\"/content/video_with_faces.mp4\")\n",
        "video_with_faces = video_with_faces.set_audio(audio_with_faces)\n",
        "video_with_faces.write_videofile(\"video_with_faces_audio.mp4\", fps=fps, codec=\"libx264\", audio_codec=\"aac\")\n",
        "\n",
        "# Set audio for video without faces\n",
        "video_without_faces = mp.VideoFileClip(\"/content/video_without_faces.mp4\")\n",
        "video_without_faces = video_without_faces.set_audio(audio_without_faces)\n",
        "video_without_faces.write_videofile(\"video_without_faces_audio.mp4\", fps=fps, codec=\"libx264\", audio_codec=\"aac\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCiY_iTqDuMq",
        "outputId": "1ef82145-be27-4bc0-ba1c-1d8433a11ab7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video video_with_faces_audio.mp4.\n",
            "MoviePy - Writing audio in video_with_faces_audioTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video video_with_faces_audio.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready video_with_faces_audio.mp4\n",
            "Moviepy - Building video video_without_faces_audio.mp4.\n",
            "MoviePy - Writing audio in video_without_faces_audioTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video video_without_faces_audio.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready video_without_faces_audio.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing and applying model"
      ],
      "metadata": {
        "id": "d4Y1L2BE_8ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/sample_data\n",
        "!mkdir /content/sample_data\n",
        "\n",
        "!git clone https://github.com/zabique/Wav2Lip\n",
        "\n",
        "#download the pretrained model\n",
        "!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O '/content/Wav2Lip/checkpoints/wav2lip_gan.pth'\n",
        "a = !pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl\n",
        "\n",
        "# !pip uninstall tensorflow tensorflow-gpu\n",
        "!cd Wav2Lip && pip install -r requirements.txt\n",
        "\n",
        "#download pretrained model for face detection\n",
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth\"\n",
        "\n",
        "!pip install -q youtube-dl\n",
        "!pip install ffmpeg-python\n",
        "!pip install librosa==0.9.1\n",
        "\n",
        "#this code for recording audio\n",
        "\"\"\"\n",
        "To write this piece of code I took inspiration/code from a lot of places.\n",
        "It was late night, so I'm not sure how much I created or just copied o.O\n",
        "Here are some of the possible references:\n",
        "https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/\n",
        "https://stackoverflow.com/a/18650249\n",
        "https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/\n",
        "https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/\n",
        "https://stackoverflow.com/a/49019356\n",
        "\"\"\"\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"\\nDone\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHtix03UAVfL",
        "outputId": "ad7d99fd-eedb-4c21-d100-31a41bfa8e59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"/content/video_with_faces_gray.mp4\" --audio \"/content/input_audio.wav\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39c-bpPKG7h5",
        "outputId": "deb94d56-356f-44e1-a8a8-fc0af526530d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda for inference.\n",
            "Reading video frames...\n",
            "Number of frames available for inference: 1262\n",
            "/content/Wav2Lip/audio.py:100: FutureWarning: Pass sr=16000, n_fft=800 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  return librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels,\n",
            "(80, 5386)\n",
            "Length of mel chunks: 2016\n",
            "  0% 0/16 [00:00<?, ?it/s]\n",
            "  0% 0/79 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 1/79 [00:43<55:56, 43.03s/it]\u001b[A\n",
            "  3% 2/79 [00:46<25:00, 19.49s/it]\u001b[A\n",
            "  4% 3/79 [00:48<14:54, 11.77s/it]\u001b[A\n",
            "  5% 4/79 [00:51<10:05,  8.08s/it]\u001b[A\n",
            "  6% 5/79 [00:53<07:31,  6.10s/it]\u001b[A\n",
            "  8% 6/79 [00:56<06:02,  4.96s/it]\u001b[A\n",
            "  9% 7/79 [00:58<05:00,  4.17s/it]\u001b[A\n",
            " 10% 8/79 [01:01<04:19,  3.65s/it]\u001b[A\n",
            " 11% 9/79 [01:04<03:51,  3.31s/it]\u001b[A\n",
            " 13% 10/79 [01:06<03:29,  3.04s/it]\u001b[A\n",
            " 14% 11/79 [01:09<03:16,  2.89s/it]\u001b[A\n",
            " 15% 12/79 [01:11<03:09,  2.83s/it]\u001b[A\n",
            " 16% 13/79 [01:14<03:00,  2.73s/it]\u001b[A\n",
            " 18% 14/79 [01:16<02:51,  2.64s/it]\u001b[A\n",
            " 19% 15/79 [01:19<02:46,  2.60s/it]\u001b[A\n",
            " 20% 16/79 [01:21<02:40,  2.55s/it]\u001b[A\n",
            " 22% 17/79 [01:24<02:37,  2.54s/it]\u001b[A\n",
            " 23% 18/79 [01:26<02:39,  2.61s/it]\u001b[A\n",
            " 24% 19/79 [01:29<02:36,  2.60s/it]\u001b[A\n",
            " 25% 20/79 [01:31<02:32,  2.58s/it]\u001b[A\n",
            " 27% 21/79 [01:34<02:27,  2.55s/it]\u001b[A\n",
            " 28% 22/79 [01:36<02:23,  2.52s/it]\u001b[A\n",
            " 29% 23/79 [01:39<02:21,  2.53s/it]\u001b[A\n",
            " 30% 24/79 [01:42<02:23,  2.61s/it]\u001b[A\n",
            " 32% 25/79 [01:44<02:22,  2.63s/it]\u001b[A\n",
            " 33% 26/79 [01:47<02:16,  2.58s/it]\u001b[A\n",
            " 34% 27/79 [01:49<02:13,  2.56s/it]\u001b[A\n",
            " 35% 28/79 [01:52<02:09,  2.53s/it]\u001b[A\n",
            " 37% 29/79 [01:54<02:06,  2.53s/it]\u001b[A\n",
            " 38% 30/79 [01:57<02:07,  2.60s/it]\u001b[A\n",
            " 39% 31/79 [02:00<02:05,  2.61s/it]\u001b[A\n",
            " 41% 32/79 [02:02<02:01,  2.57s/it]\u001b[A\n",
            " 42% 33/79 [02:05<01:56,  2.54s/it]\u001b[A\n",
            " 43% 34/79 [02:07<01:53,  2.52s/it]\u001b[A\n",
            " 44% 35/79 [02:10<01:50,  2.51s/it]\u001b[A\n",
            " 46% 36/79 [02:12<01:51,  2.58s/it]\u001b[A\n",
            " 47% 37/79 [02:15<01:49,  2.60s/it]\u001b[A\n",
            " 48% 38/79 [02:18<01:44,  2.55s/it]\u001b[A\n",
            " 49% 39/79 [02:20<01:40,  2.52s/it]\u001b[A\n",
            " 51% 40/79 [02:22<01:37,  2.49s/it]\u001b[A\n",
            " 52% 41/79 [02:25<01:34,  2.50s/it]\u001b[A\n",
            " 53% 42/79 [02:28<01:35,  2.58s/it]\u001b[A\n",
            " 54% 43/79 [02:30<01:33,  2.60s/it]\u001b[A\n",
            " 56% 44/79 [02:33<01:29,  2.55s/it]\u001b[A\n",
            " 57% 45/79 [02:35<01:25,  2.53s/it]\u001b[A\n",
            " 58% 46/79 [02:38<01:22,  2.50s/it]\u001b[A\n",
            " 59% 47/79 [02:40<01:19,  2.50s/it]\u001b[A\n",
            " 61% 48/79 [02:43<01:19,  2.58s/it]\u001b[A\n",
            " 62% 49/79 [02:46<01:18,  2.62s/it]\u001b[A\n",
            " 63% 50/79 [02:48<01:14,  2.58s/it]\u001b[A\n",
            " 65% 51/79 [02:51<01:10,  2.53s/it]\u001b[A\n",
            " 66% 52/79 [02:53<01:07,  2.50s/it]\u001b[A\n",
            " 67% 53/79 [02:56<01:05,  2.52s/it]\u001b[A\n",
            " 68% 54/79 [02:58<01:04,  2.60s/it]\u001b[A\n",
            " 70% 55/79 [03:01<01:02,  2.60s/it]\u001b[A\n",
            " 71% 56/79 [03:03<00:58,  2.55s/it]\u001b[A\n",
            " 72% 57/79 [03:06<00:55,  2.52s/it]\u001b[A\n",
            " 73% 58/79 [03:08<00:52,  2.49s/it]\u001b[A\n",
            " 75% 59/79 [03:11<00:50,  2.50s/it]\u001b[A\n",
            " 76% 60/79 [03:14<00:48,  2.57s/it]\u001b[A\n",
            " 77% 61/79 [03:16<00:46,  2.61s/it]\u001b[A\n",
            " 78% 62/79 [03:19<00:43,  2.56s/it]\u001b[A\n",
            " 80% 63/79 [03:21<00:40,  2.53s/it]\u001b[A\n",
            " 81% 64/79 [03:24<00:37,  2.51s/it]\u001b[A\n",
            " 82% 65/79 [03:26<00:35,  2.50s/it]\u001b[A\n",
            " 84% 66/79 [03:29<00:33,  2.58s/it]\u001b[A\n",
            " 85% 67/79 [03:32<00:31,  2.61s/it]\u001b[A\n",
            " 86% 68/79 [03:34<00:28,  2.59s/it]\u001b[A\n",
            " 87% 69/79 [03:38<00:29,  2.90s/it]\u001b[A\n",
            " 89% 70/79 [03:42<00:30,  3.34s/it]\u001b[A\n",
            " 90% 71/79 [03:46<00:28,  3.58s/it]\u001b[A\n",
            " 91% 72/79 [03:50<00:25,  3.60s/it]\u001b[A\n",
            " 92% 73/79 [03:54<00:22,  3.78s/it]\u001b[A\n",
            " 94% 74/79 [03:59<00:20,  4.04s/it]\u001b[A\n",
            " 95% 75/79 [04:02<00:14,  3.67s/it]\u001b[A\n",
            " 96% 76/79 [04:04<00:09,  3.29s/it]\u001b[A\n",
            " 97% 77/79 [04:06<00:06,  3.03s/it]\u001b[A\n",
            " 99% 78/79 [04:09<00:02,  2.86s/it]\u001b[A\n",
            "100% 79/79 [04:40<00:00,  3.55s/it]\n",
            "Load checkpoint from: checkpoints/wav2lip_gan.pth\n",
            "Model loaded\n",
            "100% 16/16 [05:19<00:00, 19.98s/it]\n",
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "\u001b[0;35m[mp3 @ 0x5c644b9c90c0] \u001b[0m\u001b[0;33mEstimating duration from bitrate, this may be inaccurate\n",
            "\u001b[0mInput #0, mp3, from '/content/input_audio.wav':\n",
            "  Duration: 00:01:07.32, start: 0.000000, bitrate: 96 kb/s\n",
            "    Stream #0:0: Audio: mp3, 44100 Hz, mono, fltp, 96 kb/s\n",
            "Input #1, avi, from 'temp/result.avi':\n",
            "  Metadata:\n",
            "    encoder         : Lavf59.27.100\n",
            "  Duration: 00:01:07.20, start: 0.000000, bitrate: 2510 kb/s\n",
            "    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 1280x640 [SAR 1:1 DAR 2:1], 2505 kb/s, 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
            "  Stream #0:0 -> #0:1 (mp3 (mp3float) -> aac (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n",
            "\u001b[0m\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'results/result_voice.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p(progressive), 1280x640 [SAR 1:1 DAR 2:1], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, mono, fltp, 69 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54.100 aac\n",
            "frame= 2016 fps= 25 q=-1.0 Lsize=   10377kB time=00:01:07.33 bitrate=1262.4kbits/s speed=0.839x    \n",
            "video:9742kB audio:571kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.623543%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mframe I:12    Avg QP:16.41  size: 31937\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mframe P:1474  Avg QP:21.24  size:  5542\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mframe B:530   Avg QP:24.41  size:  2685\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mconsecutive B-frames: 59.7% 11.4% 13.2% 15.7%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mmb I  I16..4: 28.5% 64.5%  7.0%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mmb P  I16..4:  1.5%  7.1%  0.2%  P16..4: 25.9%  5.3%  2.1%  0.0%  0.0%    skip:57.9%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mmb B  I16..4:  0.5%  2.4%  0.0%  B16..8: 31.1%  2.8%  0.2%  direct: 0.8%  skip:62.1%  L0:55.1% L1:41.2% BI: 3.7%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0m8x8 transform intra:79.4% inter:82.4%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mcoded y,uvDC,uvAC intra: 28.8% 30.2% 4.4% inter: 6.3% 5.6% 0.0%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mi16 v,h,dc,p: 45% 26% 21%  8%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 18% 47%  2%  1%  2%  2%  1%  2%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 25% 13%  3%  6%  7%  7%  4%  3%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mi8c dc,h,v,p: 59% 18% 20%  2%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mWeighted P-Frames: Y:0.3% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mref P L0: 73.2% 10.0% 12.2%  4.5%  0.0%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mref B L0: 82.5% 14.3%  3.2%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mref B L1: 97.6%  2.4%\n",
            "\u001b[1;36m[libx264 @ 0x5c644ba14680] \u001b[0mkb/s:1187.47\n",
            "\u001b[1;36m[aac @ 0x5c644ba126c0] \u001b[0mQavg: 158.364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/Wav2Lip/results/result_voice.mp4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-SMXFx6ITqaK",
        "outputId": "47f0cf4d-6086-4303-b16b-2944e2392de4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5aa7441d-4332-4420-b1d6-63cd6d853196\", \"result_voice.mp4\", 10625822)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rough work"
      ],
      "metadata": {
        "id": "R3dBraiZ9pdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import face_recognition\n",
        "\n",
        "# # Mount Google Drive to access video file\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Load the video\n",
        "video_path = \"/content/input_video.mp4\"  # Update with your video file path\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Create lists to store frames\n",
        "frames_with_faces = []\n",
        "frames_without_faces = []\n",
        "\n",
        "# # Enable CUDA for GPU acceleration\n",
        "# cv2.cuda.setDevice(0)  # Set the GPU device number, if multiple GPUs are available\n",
        "\n",
        "while True:\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to RGB for face_recognition library\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    face_locations = face_recognition.face_locations(rgb_frame)\n",
        "\n",
        "    if len(face_locations) > 0:\n",
        "        # Store frames with faces\n",
        "        frames_with_faces.append(frame)\n",
        "    else:\n",
        "        # Store frames without faces\n",
        "        frames_without_faces.append(frame)\n",
        "\n",
        "# Create video writer for frames with faces\n",
        "writer_with_faces = cv2.VideoWriter(\"/content/video_with_faces.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (frames_with_faces[0].shape[1], frames_with_faces[0].shape[0]))\n",
        "for frame in frames_with_faces:\n",
        "    writer_with_faces.write(frame)\n",
        "writer_with_faces.release()\n",
        "\n",
        "# Create video writer for frames without faces\n",
        "writer_without_faces = cv2.VideoWriter(\"/content/video_without_faces.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (frames_without_faces[0].shape[1], frames_without_faces[0].shape[0]))\n",
        "for frame in frames_without_faces:\n",
        "    writer_without_faces.write(frame)\n",
        "writer_without_faces.release()\n",
        "\n",
        "# Release the video object\n",
        "video.release()"
      ],
      "metadata": {
        "id": "1lZFgNbmMsQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with timestamp"
      ],
      "metadata": {
        "id": "TWPj440HmvJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import face_recognition\n",
        "import numpy as np\n",
        "\n",
        "# Load the video\n",
        "video_path = \"/content/input_video.mp4\"  # Update with your video file path\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Create lists to store frames and timestamps\n",
        "frames_with_faces = []\n",
        "timestamps_with_faces = []\n",
        "frames_without_faces = []\n",
        "timestamps_without_faces = []\n",
        "\n",
        "# # Enable CUDA for GPU acceleration\n",
        "# cv2.cuda.setDevice(0)  # Set the GPU device number, if multiple GPUs are available\n",
        "\n",
        "while True:\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to RGB for face_recognition library\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # # Transfer frame data to GPU memory\n",
        "    # gpu_frame = cv2.cuda_GpuMat()\n",
        "    # gpu_frame.upload(rgb_frame)\n",
        "\n",
        "    # Detect faces in the frame using GPU-accelerated face recognition\n",
        "    face_locations = face_recognition.face_locations(rgb_frame)\n",
        "\n",
        "    if len(face_locations) > 0:\n",
        "        # Store frames with faces and corresponding timestamps\n",
        "        frames_with_faces.append(frame)\n",
        "        timestamps_with_faces.append(video.get(cv2.CAP_PROP_POS_MSEC) / 1000.0)\n",
        "    else:\n",
        "        # Store frames without faces and corresponding timestamps\n",
        "        frames_without_faces.append(frame)\n",
        "        timestamps_without_faces.append(video.get(cv2.CAP_PROP_POS_MSEC) / 1000.0)\n",
        "\n",
        "# Create video writer for frames with faces\n",
        "writer_with_faces = cv2.VideoWriter(\"/content/video_with_faces.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (frames_with_faces[0].shape[1], frames_with_faces[0].shape[0]))\n",
        "for frame in frames_with_faces:\n",
        "    writer_with_faces.write(frame)\n",
        "writer_with_faces.release()\n",
        "\n",
        "# Create video writer for frames without faces\n",
        "writer_without_faces = cv2.VideoWriter(\"/content/video_without_faces.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (frames_without_faces[0].shape[1], frames_without_faces[0].shape[0]))\n",
        "for frame in frames_without_faces:\n",
        "    writer_without_faces.write(frame)\n",
        "writer_without_faces.release()\n",
        "\n",
        "# Save timestamps of frames with faces\n",
        "np.savetxt(\"timestamps_with_faces.txt\", timestamps_with_faces)\n",
        "\n",
        "# Save timestamps of frames without faces\n",
        "np.savetxt(\"timestamps_without_faces.txt\", timestamps_without_faces)\n",
        "\n",
        "# Release the video object\n",
        "video.release()"
      ],
      "metadata": {
        "id": "Bzod0_D-mp-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd (txt to speech) NOT ATTEMPTING"
      ],
      "metadata": {
        "id": "_NuAj4CN_5bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install tensorflow-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoFZZJVO2MvJ",
        "outputId": "01ea1cad-2a79-4051-a4ca-7a66327d4261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inflect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3JFzzQg3GHB",
        "outputId": "59c3986f-8e53-4d06-a89a-4843eaad0530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (6.0.5)\n",
            "Requirement already satisfied: pydantic<2,>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect) (1.10.11)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1.9.1->inflect) (4.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZhq1YH93PDm",
        "outputId": "ba383dc5-787f-49b1-acd7-81b3ef0a0ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.0.post2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.7.1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0eiwBT73b5d",
        "outputId": "ad8f20b7-97dd-46b0-8e41-cef52bf025f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaAk5K4M3529",
        "outputId": "f1c97b85-177f-41f0-b054-3956d1ca1d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "kaJbBjYU2Q0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the Tacotron 2 repository\n",
        "!git clone https://github.com/NVIDIA/tacotron2.git\n",
        "%cd tacotron2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixqgHZE52OsQ",
        "outputId": "497c1fbd-c01b-4c0f-ae03-e51f2c0acc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tacotron2'...\n",
            "remote: Enumerating objects: 412, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 412 (delta 2), reused 3 (delta 0), pack-reused 406\u001b[K\n",
            "Receiving objects: 100% (412/412), 2.70 MiB | 3.13 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n",
            "/content/tacotron2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pre-trained model checkpoints (e.g., Tacotron 2 and WaveGlow)\n",
        "!wget <link_to_tacotron2_checkpoint>\n",
        "!wget <link_to_waveglow_checkpoint>"
      ],
      "metadata": {
        "id": "G5rcuiqt2SVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZMPvwr414eg"
      },
      "outputs": [],
      "source": [
        "# Run the inference script\n",
        "!python inference.py --tacotron2 <path_to_tacotron2_checkpoint> --waveglow <path_to_waveglow_checkpoint> --text \"Hello, how are you?\""
      ]
    }
  ]
}